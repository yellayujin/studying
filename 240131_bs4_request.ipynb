{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellayujin/studying/blob/main/240131_bs4_request.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c26d5d5-af3e-47ff-869e-06a8ebfd0b66",
      "metadata": {
        "id": "4c26d5d5-af3e-47ff-869e-06a8ebfd0b66",
        "outputId": "8c7e4186-003a-4b84-c390-3159a8ab9426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.31.0\n",
            "4.12.3\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import bs4\n",
        "\n",
        "print(requests.__version__)\n",
        "print(bs4.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ce4343-8e6c-47ee-bf48-71db5d94a1dc",
      "metadata": {
        "id": "d5ce4343-8e6c-47ee-bf48-71db5d94a1dc"
      },
      "source": [
        "## 네이버 상태 코드 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c04bbb-f6bf-4ab9-bfa8-a0d273cc6023",
      "metadata": {
        "id": "96c04bbb-f6bf-4ab9-bfa8-a0d273cc6023",
        "outputId": "18cf8cb5-97f1-4d26-f8ff-7d07a3d6f507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n"
          ]
        }
      ],
      "source": [
        "URL = 'http://www.naver.com/'\n",
        "\n",
        "req = requests.get(URL) # 검사 -> Network -> 목록 중 선택 -> Headers 등 선택하면 request method가 GET으로 나와있음 (POST방식이면 코드 조금 다르니 공식문서 찾아보길)\n",
        "print(req.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3376382a-af80-43ee-94de-fa14e03d6450",
      "metadata": {
        "id": "3376382a-af80-43ee-94de-fa14e03d6450"
      },
      "outputs": [],
      "source": [
        "# req.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bf67321-796b-4c0b-9805-0dfff15ed94d",
      "metadata": {
        "id": "4bf67321-796b-4c0b-9805-0dfff15ed94d",
        "outputId": "f9147525-8d4a-41fa-ad7f-905487055621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<h2>모바일 컴퍼니</h2>\n",
            "--------\n",
            "<ul id=\"mylist\" style=\"width:150px\">\n",
            "<li>애플</li>\n",
            "<li>삼성</li>\n",
            "<li>노키아</li>\n",
            "<li>LG</li>\n",
            "</ul>\n",
            "--------\n",
            "<li>애플</li>\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# step 01: 데이터 수집\n",
        "with open('index.html', 'r', encoding='UTF-8') as f:\n",
        "    contents = f.read()\n",
        "\n",
        "# step 02: 데이터 파싱(=순수한 html파일(문자열 형태)을 BeautifulSoup 객체로 변환 for 태그에 접근)\n",
        "    soup = BeautifulSoup(contents, 'lxml')\n",
        "\n",
        "    # print(soup)\n",
        "    print(soup.h2)      # <h2>태그부분만 나옴\n",
        "    print('--------')\n",
        "    print(soup.ul)\n",
        "    print('--------')\n",
        "    print(soup.ul.li)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2852d81f-6d62-4dea-98e9-31ae80db63c4",
      "metadata": {
        "id": "2852d81f-6d62-4dea-98e9-31ae80db63c4",
        "outputId": "ab5b04ab-7de2-4317-c988-ab76bf6a67a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<li>애플</li>\n"
          ]
        }
      ],
      "source": [
        "print(soup.li)  # 위 문서에서 li코드 4개인데 하나만 나옴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "386e0d87-8082-481f-90ea-2f5bc6efee3c",
      "metadata": {
        "id": "386e0d87-8082-481f-90ea-2f5bc6efee3c",
        "outputId": "11ace782-97f7-4f4a-d6e8-779be9ee11b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['애플', '삼성', '노키아', 'LG']\n"
          ]
        }
      ],
      "source": [
        "# step 03: 데이터 수집을 위한 특정 태그 찾기\n",
        "# 4개의 li태그에 있는 회사명을 모두 가져오는 것이 목적\n",
        "companies = []\n",
        "for tag in soup.find_all('li'):   # step 04: 데이터 가공\n",
        "    companies.append(tag.text)   # .text로 태그는 안 가져오고 텍스트만 가져옴\n",
        "\n",
        "print(companies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84b7b8ea-c4ad-41d1-9146-7d6d5d8a6ec7",
      "metadata": {
        "id": "84b7b8ea-c4ad-41d1-9146-7d6d5d8a6ec7",
        "outputId": "303bc501-d3fa-44ce-e036-53b4910f02b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   회사명\n",
            "0   애플\n",
            "1   삼성\n",
            "2  노키아\n",
            "3   LG\n"
          ]
        }
      ],
      "source": [
        "# step 05: 처리된 데이터 저장 pandas 데이터프레임\n",
        "import pandas as pd\n",
        "crawling_dict = {'회사명': companies}\n",
        "result = pd.DataFrame(crawling_dict)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c483c4b2-bddb-4040-a4a6-9c8ef3b886b9",
      "metadata": {
        "id": "c483c4b2-bddb-4040-a4a6-9c8ef3b886b9"
      },
      "outputs": [],
      "source": [
        "# step 06: csv (or DB)로 내보내기)\n",
        "result.to_csv('result.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f09852e-91d0-4dbd-946a-bb92c62c99c9",
      "metadata": {
        "id": "8f09852e-91d0-4dbd-946a-bb92c62c99c9"
      },
      "outputs": [],
      "source": [
        "# 벅스\n",
        "custom_header = {\n",
        "\t'referer' : 'https://music.bugs.co.kr/',\n",
        "\t'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "}\n",
        "url = \"https://music.bugs.co.kr/chart\" # 크롤링 하려는 웹사이트\n",
        "req = requests.get(url, headers = custom_header)\n",
        "\n",
        "data = req.text\n",
        "soup = BeautifulSoup(data, 'lxml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34af9268-9415-4df0-842b-0b996ea6a59e",
      "metadata": {
        "id": "34af9268-9415-4df0-842b-0b996ea6a59e",
        "outputId": "907e85ef-4e32-47f1-ca21-6d45726491d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Love wins all', 'Wife', 'To. X', 'Love 119', '에피소드', 'Yes or No (Feat. 허윤진 of LE SSERAFIM, Crush)', 'MANIAC', 'Perfect Night', '비의 랩소디', 'Drama', 'Get A Guitar', '첫 만남은 계획대로 되지 않아', 'DASH', 'You & Me', 'I AM', '그대만 있다면 (여름날 우리 X 너드커넥션 (Nerd Connection))', 'Ditto', '한 페이지가 될 수 있게', 'Super Lady', 'Baddie', '헤어지자 말해요', '인사', 'ETA', '꿈', 'Hype Boy', '안 아름답고도 안 아프구나', '후라이의 꿈', 'Super Shy', 'Love Lee', 'Talk Saxy', 'Steal The Show (From “엘리멘탈”)', '그대가 내 안에 박혔다', 'Discord', 'Seven (feat. Latto) - Clean Ver.', 'Standing Next to You', '첫 눈', '숲', '너의 모든 순간', 'GODS', '예뻤어', 'Spicy', 'Attention', '넌 쉽게 말했지만', 'Chill Kill', 'Off The Record', '모든 날, 모든 순간 (Every day, Every Moment)', 'Kitsch', '별 떨어진다 (I Do)', '잠시라도 우리', 'New Jeans', '어떻게 이별까지 사랑하겠어, 널 사랑하는 거지', '퀸카 (Queencard)', 'Off My Face', 'Dangerously', '사건의 지평선', 'OMG', 'I Don’t Think That I Like Her', '사랑인가 봐', '새벽 통화', 'LOVE DIVE', '이브, 프시케 그리고 푸른 수염의 아내', '잘 지내자, 우리 (여름날 우리 X 로이킴)', '사막에서 꽃을 피우듯', 'UNFORGIVEN (feat. Nile Rodgers)', '어른', 'Underwater', 'STAY', '취중고백', '주저하는 연인들을 위해', '심(心)', 'After LIKE', '다정히 내 이름을 부르면', '파이팅 해야지 (Feat. 이영지)', 'Fast Forward', '사랑은 늘 도망가', '건물 사이에 피어난 장미 (Rose Blossom)', 'When I Get Old', '사랑할 수밖에', '달빛에 그려지는', '손오공', '음악의 신', '사랑을 하다가', '한번만 더', 'Try Again', 'Either Way', 'Bubble', '화이트 (White)', 'ANTIFRAGILE', 'Smoke (Prod. Dynamicduo, Padi)', 'DIE 4 YOU', '그대가 내 안에 박혔다(그내박)', 'Summer (Feat. BE’O (비오))', '밤, 바다', 'I Love You', '기억해줘요 내 모든 날과 그때를', '머물러주오 (Prod. 안신애 & Philtre)', '운이 좋았지', 'Snowman', 'Cool With You', '사랑은 먼 길을 돌아온 메아리 같아서']\n"
          ]
        }
      ],
      "source": [
        "song_list = []\n",
        "for tag in soup.find_all('p', class_='title'):\n",
        "    song_list.append(tag.text.strip('\\n'))\n",
        "\n",
        "print(song_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955b90e6-6c5a-4470-9cef-d30b315577fd",
      "metadata": {
        "id": "955b90e6-6c5a-4470-9cef-d30b315577fd",
        "outputId": "1b17adaf-466d-4896-e326-d58db0a9109b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           곡 제목\n",
            "0                 Love wins all\n",
            "1                          Wife\n",
            "2                         To. X\n",
            "3                      Love 119\n",
            "4                          에피소드\n",
            "..                          ...\n",
            "95  머물러주오 (Prod. 안신애 & Philtre)\n",
            "96                       운이 좋았지\n",
            "97                      Snowman\n",
            "98                Cool With You\n",
            "99         사랑은 먼 길을 돌아온 메아리 같아서\n",
            "\n",
            "[100 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "song_dict = {'곡 제목':  song_list}\n",
        "result = pd.DataFrame(song_dict)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03974d1b-2bab-4b21-ba92-b38ae415231a",
      "metadata": {
        "id": "03974d1b-2bab-4b21-ba92-b38ae415231a",
        "outputId": "10d412a4-ab39-4dd8-a8d9-7e7da3604e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                              노래제목\n",
            "0                    Love wins all\n",
            "1                             Wife\n",
            "2                         Love 119\n",
            "3                            To. X\n",
            "4                             에피소드\n",
            "..                             ...\n",
            "95                         사랑을 하다가\n",
            "96  Smoke (Prod. Dynamicduo, Padi)\n",
            "97                        미워 (Ego)\n",
            "98                           밤, 바다\n",
            "99            사랑은 먼 길을 돌아온 메아리 같아서\n",
            "\n",
            "[100 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "# 강사\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def crawling(soup) :\n",
        "    # print(soup)\n",
        "    tbody = soup.find(\"tbody\")   # 지금 사이트에서는 table이 한 개지만, 여러 개일 경우 tbody부터 찾아야 해서 이렇게 작성\n",
        "    result = []\n",
        "    for p in tbody.find_all('p', class_ = 'title'):   # class라고만 쓰면 클래스 정의할 때 그 클래스랑 겹쳐서 class_로 정의함\n",
        "        result.append(p.get_text().strip())\n",
        "    return result\n",
        "\n",
        "def main() :\n",
        "    custom_header = {\n",
        "        'referer' : 'https://music.bugs.co.kr/',\n",
        "        'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "    }\n",
        "    url = \"https://music.bugs.co.kr/chart\" # 크롤링 하려는 웹사이트\n",
        "    req = requests.get(url, headers = custom_header)\n",
        "\n",
        "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "    crawling(soup)\n",
        "\n",
        "    titles = crawling(soup)\n",
        "    print(pd.DataFrame({\"노래제목\" : titles}))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba703264-c215-41a6-8917-ed12ef28f3b3",
      "metadata": {
        "id": "ba703264-c215-41a6-8917-ed12ef28f3b3"
      },
      "outputs": [],
      "source": [
        "# 주식 힌트\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "import time\n",
        "import random\n",
        "\n",
        "company_code = '005930' # 삼성전자\n",
        "url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code   # 여기에 &page=숫자 들어감\n",
        "\n",
        "headers = {\n",
        "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
        "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "            }\n",
        "req = requests.get(url, headers=headers)\n",
        "soup = BeautifulSoup(req.content, \"html.parser\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4715de34-85b9-4895-bda1-c85a13ca308e",
      "metadata": {
        "id": "4715de34-85b9-4895-bda1-c85a13ca308e",
        "outputId": "f7e8f5e7-0b8f-4dd0-953a-cc9aa17abeac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\skrtk\\AppData\\Local\\Temp\\ipykernel_9772\\4192256325.py:1: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
            "  pd.read_html(req.text, encoding='euc-kr')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[            날짜       종가     전일비       시가       고가       저가         거래량\n",
              " 0          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
              " 1   2024.01.31  72800.0  1500.0  73400.0  74000.0  72600.0  12046907.0\n",
              " 2   2024.01.30  74300.0   100.0  75000.0  75300.0  73700.0  12244418.0\n",
              " 3   2024.01.29  74400.0  1000.0  73800.0  75200.0  73500.0  13976521.0\n",
              " 4   2024.01.26  73400.0   700.0  73700.0  74500.0  73300.0  11160062.0\n",
              " 5   2024.01.25  74100.0   100.0  74200.0  74800.0  73700.0  11737747.0\n",
              " 6          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
              " 7          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
              " 8          NaN      NaN     NaN      NaN      NaN      NaN         NaN\n",
              " 9   2024.01.24  74000.0  1200.0  75200.0  75200.0  73500.0  12860661.0\n",
              " 10  2024.01.23  75200.0   100.0  75700.0  75800.0  74300.0  14786224.0\n",
              " 11  2024.01.22  75100.0   400.0  75900.0  76000.0  75000.0  19673375.0\n",
              " 12  2024.01.19  74700.0  3000.0  73500.0  74700.0  73000.0  23363427.0\n",
              " 13  2024.01.18  71700.0   700.0  71600.0  72000.0  70700.0  17853397.0\n",
              " 14         NaN      NaN     NaN      NaN      NaN      NaN         NaN,\n",
              "    0   1   2   3   4   5   6   7   8   9   10  11\n",
              " 0   1   2   3   4   5   6   7   8   9  10  다음  맨뒤]"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.read_html(req.text, encoding='euc-kr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8641ba-e22a-4d4d-8d91-0d7f7e567238",
      "metadata": {
        "id": "dc8641ba-e22a-4d4d-8d91-0d7f7e567238",
        "outputId": "29e746ed-2b85-4cce-82e2-fab28680d255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "             날짜       종가     전일비       시가       고가       저가         거래량\n",
            "0    2024.01.31  72700.0  1600.0  73400.0  74000.0  72500.0  13080752.0\n",
            "1    2024.01.30  74300.0   100.0  75000.0  75300.0  73700.0  12244418.0\n",
            "2    2024.01.29  74400.0  1000.0  73800.0  75200.0  73500.0  13976521.0\n",
            "3    2024.01.26  73400.0   700.0  73700.0  74500.0  73300.0  11160062.0\n",
            "4    2024.01.25  74100.0   100.0  74200.0  74800.0  73700.0  11737747.0\n",
            "..          ...      ...     ...      ...      ...      ...         ...\n",
            "115  2023.08.10  68000.0   900.0  68300.0  68500.0  67800.0  10227311.0\n",
            "116  2023.08.09  68900.0  1300.0  68000.0  69600.0  67900.0  17259673.0\n",
            "117  2023.08.08  67600.0   900.0  69000.0  69100.0  67400.0  14664709.0\n",
            "118  2023.08.07  68500.0   200.0  67700.0  69200.0  67600.0  10968505.0\n",
            "119  2023.08.04  68300.0   500.0  68800.0  69100.0  68200.0  12360193.0\n",
            "\n",
            "[120 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings\n",
        "import time\n",
        "import random\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def crawling(url, headers, soup):\n",
        "    last_page = int(soup.select_one('td.pgRR').a['href'].split('=')[-1])\n",
        "\n",
        "    df = None\n",
        "    count = 0\n",
        "    for page in range(1, last_page + 1):\n",
        "      req = requests.get(f'{url}&page={page}', headers=headers)\n",
        "      df = pd.concat([df, pd.read_html(req.text, encoding = \"euc-kr\")[0]], ignore_index=True)\n",
        "      if count > 10:\n",
        "        break\n",
        "      count += 1\n",
        "      time.sleep( random.uniform(2,4))\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    company_code = '005930' # 삼성전자\n",
        "    url =\"https://finance.naver.com/item/sise_day.nhn?code=\" + company_code\n",
        "\n",
        "    headers = {\n",
        "             'referer' : 'https://finance.naver.com/item/sise.naver?code=005930',\n",
        "             'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
        "            }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    result = crawling(url, headers, soup)\n",
        "    print(result)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}